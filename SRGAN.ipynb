{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Teaching_SRGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fevDTR3Wcy2s"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBvDQm-4cxge"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkHZHqclIxWA"
      },
      "source": [
        "# Colab\n",
        "If you are planning any changes or modifications please use the github link: https://github.com/monishramadoss/SRGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NILfvFIxns29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "1d5748d8-4966-46e3-8ca3-d1ac04734599"
      },
      "source": [
        "!nvidia-smi\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr  8 02:08:22 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Collecting https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[?25l  Downloading https://github.com/chengs/tqdm/archive/colab.zip (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 533kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: tqdm\n",
            "  Building wheel for tqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tqdm: filename=tqdm-4.28.1-py2.py3-none-any.whl size=47867 sha256=e8fa5355da5afec2c8fb97308fc367c196d429ab0abcab22160a4a6808969d8f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8g9w8ss3/wheels/41/18/ee/d5dd158441b27965855b1bbae03fa2d8a91fe645c01b419896\n",
            "Successfully built tqdm\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.28.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.38.0\n",
            "    Uninstalling tqdm-4.38.0:\n",
            "      Successfully uninstalled tqdm-4.38.0\n",
            "Successfully installed tqdm-4.28.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsFNc7CJhjbS"
      },
      "source": [
        "# HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkLWt8ijhm4k"
      },
      "source": [
        "UPSCALE_FACTOR = 2\n",
        "BATCHSIZE = 32\n",
        "EPOCHS = 1000\n",
        "LOWRES = 56"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxUfizQ9fJ48"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUlqBYAhez0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "4e1f063f-ad24-4b91-f143-043dfc4173c4"
      },
      "source": [
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "DEVSET_URL = \"http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip\"\n",
        "TRAINSET_URL = \"http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\"\n",
        "DEVSET = \"./data/DIV2K_valid_HR.zip\"\n",
        "TRAINSET = \"./data/DIV2K_train_HR.zip\"\n",
        "DEVDATA_FOLDER = \"./data/DIV2K_valid_HR\"\n",
        "TRAINDATA_FOLDER = \"./data/DIV2K_train_HR\"\n",
        "\n",
        "class TqdmUpTo(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "\n",
        "if not os.path.exists('./data'):\n",
        "    os.makedirs('./data')\n",
        "if not os.path.exists('./data/train'):\n",
        "    os.makedirs('./data/train')\n",
        "if not os.path.exists('./data/dev'):\n",
        "    os.makedirs('./data/dev')\n",
        "if not os.path.exists('./checkpoint'):\n",
        "    os.makedirs('./checkpoint')\n",
        "\n",
        "\n",
        "if not os.path.exists(TRAINSET):\n",
        "    with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=\"Div2k Train Set\") as t:\n",
        "        urllib.request.urlretrieve(TRAINSET_URL, TRAINSET, reporthook=t.update_to)\n",
        "    with zipfile.ZipFile(TRAINSET, 'r') as zip_ref:\n",
        "        zip_ref.extractall('./data/train')\n",
        "if not os.path.exists(DEVSET):\n",
        "    with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=\"Div2k Valid Set\") as t:\n",
        "        urllib.request.urlretrieve(DEVSET_URL, DEVSET, reporthook=t.update_to)\n",
        "    with zipfile.ZipFile(DEVSET, 'r') as zip_ref:\n",
        "        zip_ref.extractall('./data/dev')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span></span><progress style='margin:2px 4px;' max='1' value='1'></progress>"
            ],
            "text/plain": [
              "[============================================================] 1/1"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>Div2k Train Set</span><progress style='margin:2px 4px;description_width:initial;' max='1' value='1'></progress>3.53GB [02:56, 20.0MB/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span></span><progress style='margin:2px 4px;' max='1' value='1'></progress>"
            ],
            "text/plain": [
              "[============================================================] 1/1"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>Div2k Valid Set</span><progress style='margin:2px 4px;description_width:initial;' max='1' value='1'></progress>449MB [00:22, 20.1MB/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYtvrVkZgMM_"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4lsbLZ9feKe"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, feature_layer=11):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        cnn = torchvision.models.vgg19(pretrained=True)\n",
        "        self.features = nn.Sequential(*list(cnn.features.children())[:(feature_layer + 1)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, 3, stride=2, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = nn.Conv2d(256, 256, 3, stride=2, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.conv7 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(512)\n",
        "        self.conv8 = nn.Conv2d(512, 512, 3, stride=2, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(512)\n",
        "        self.conv9 = nn.Conv2d(512, 1, 1, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = swish(self.conv1(x))\n",
        "        x = swish(self.bn2(self.conv2(x)))\n",
        "        x = swish(self.bn3(self.conv3(x)))\n",
        "        x = swish(self.bn4(self.conv4(x)))\n",
        "        x = swish(self.bn5(self.conv5(x)))\n",
        "        x = swish(self.bn6(self.conv6(x)))\n",
        "        x = swish(self.bn7(self.conv7(x)))\n",
        "        x = swish(self.bn8(self.conv8(x)))\n",
        "        x = self.conv9(x)\n",
        "\n",
        "        return torch.sigmoid(F.avg_pool2d(x, x.size()[2:])).view(x.size()[0], -1)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, n_residual_blocks, upscale_factor=2, n_filters=64, inplace=False):\n",
        "        super(Generator, self).__init__()\n",
        "        self.n_residual_blocks = n_residual_blocks\n",
        "        self.upsample_factor = upscale_factor\n",
        "        self.conv1 = nn.Conv2d(3, n_filters, 9, stride=1, padding=4)\n",
        "\n",
        "        for i in range(self.n_residual_blocks):\n",
        "            self.add_module('residual_block' + str(i + 1), ResidualBlock(n_filters, 3, n_filters, 1))\n",
        "\n",
        "        self.conv2 = nn.Conv2d(n_filters, n_filters, 3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(n_filters)\n",
        "        for i in range(self.upsample_factor // 2):\n",
        "            self.add_module('upsample' + str(i + 1), UpsampleBlock(n_filters, n_filters))\n",
        "        self.conv3 = nn.Conv2d(n_filters, 3, 9, stride=1, padding=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = swish(self.conv1(x))\n",
        "        y = x.clone()\n",
        "\n",
        "        for i in range(self.n_residual_blocks):\n",
        "            y = self.__getattr__('residual_block' + str(i + 1))(y)\n",
        "\n",
        "        x = self.bn2(self.conv2(y)) + x\n",
        "\n",
        "        for i in range(self.upsample_factor // 2):\n",
        "            x = self.__getattr__('upsample' + str(i + 1))(x)\n",
        "\n",
        "        return self.conv3(x)\n",
        "\n",
        "\n",
        "class UpSampleConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=2):\n",
        "        super(UpSampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        self.upsample_layer = nn.Upsample(scale_factor=upsample)\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.upsample_layer(x)\n",
        "        y = self.reflection_pad(y)\n",
        "        y = self.conv(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels=64, k=3, n=64, s=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, n, k, stride=s, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(n)\n",
        "        self.conv2 = nn.Conv2d(n, n, k, stride=s, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.bn1(y)\n",
        "        y = swish(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.bn2(y)\n",
        "        y = y + x\n",
        "        return y\n",
        "\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UpsampleBlock, self).__init__()\n",
        "        # self.conv = nn.Conv2d(in_channels, out_channels * 4, 3, 1, padding=1)\n",
        "        self.convT = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        # self.up_layer = UpSampleConvLayer(in_channels, out_channels, 3, 1)\n",
        "        # self.shuffler = nn.PixelShuffle(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # y = self.up_layer(x)\n",
        "        # y = self.conv(x)\n",
        "        y = self.convT(x)\n",
        "        # y = self.shuffler(y)\n",
        "        y = self.bn(y)\n",
        "        y = swish(y)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yii2CCxKgRyK"
      },
      "source": [
        "# Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7v0dKOhgQT4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "eda9035d-d054-45fa-80fb-6048e5c08db2"
      },
      "source": [
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.optim as optim\n",
        "\n",
        "transform = transforms.Compose([transforms.RandomCrop(LOWRES*UPSCALE_FACTOR),\n",
        "                                transforms.ToTensor()])\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "unnormalize = transforms.Normalize(mean = [-2.118, -2.036, -1.804], std = [4.367, 4.464, 4.444])\n",
        "scale = transforms.Compose([transforms.ToPILImage(), transforms.Resize(LOWRES),\n",
        "                            transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "train_dataset = datasets.ImageFolder(root='./data/train', transform=transform)\n",
        "dev_dataset = datasets.ImageFolder(root='./data/dev', transform=transform)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, BATCHSIZE)\n",
        "valid_dataloader = torch.utils.data.DataLoader(dev_dataset, 1)\n",
        "\n",
        "\n",
        "content_criterion = nn.MSELoss()\n",
        "GeneratorDevice = torch.device(\"cuda:0\")\n",
        "DiscriminatorDevice = torch.device(\"cuda:0\")\n",
        "adversarial_criterion = nn.BCELoss()\n",
        "\n",
        "generator = Generator(16, UPSCALE_FACTOR)\n",
        "discriminator = Discriminator()\n",
        "feature_extractor = FeatureExtractor()\n",
        "\n",
        "generator = generator.to(GeneratorDevice)\n",
        "discriminator = discriminator.to(DiscriminatorDevice)\n",
        "feature_extractor = feature_extractor.to(DiscriminatorDevice)\n",
        "low_res = torch.FloatTensor(BATCHSIZE, 3, LOWRES, LOWRES)\n",
        "ones_const = torch.ones(BATCHSIZE, 1).to(DiscriminatorDevice)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='574673361' value='574673361'></progress>100% 548M/548M [00:20&lt;00:00, 27.5MB/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIGLsl_DEcjN"
      },
      "source": [
        "# Pretrain Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFVgl1a3EcRa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "83d6d1ec-6eca-4109-a174-bee100d635a3"
      },
      "source": [
        "optim_generator = optim.Adam(generator.parameters(), lr=0.0001)\n",
        "for epoch in tqdm(range(2), desc ='Pretraining'):\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        high_res_real, _ = data\n",
        "        for j in range(BATCHSIZE):\n",
        "            low_res[j] = scale(high_res_real[j])\n",
        "            high_res_real[j] = normalize(high_res_real[j])\n",
        "        high_res_real = high_res_real.to(GeneratorDevice)\n",
        "        high_res_fake = generator(low_res.to(GeneratorDevice))\n",
        "\n",
        "        generator.zero_grad()\n",
        "        generator_content_loss = content_criterion(high_res_fake, high_res_real)\n",
        "        generator_content_loss.backward()\n",
        "        optim_generator.step()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>Pretraining</span><progress style='margin:2px 4px;description_width:initial;' max='2' value='2'></progress>100% 2/2 [03:13&lt;00:00, 97.14s/it]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6rnglmsGgOX"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd7KKEokuPgP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "f2c899c8-4e1e-4f84-c451-8421e0a4fbba"
      },
      "source": [
        "generator_optimzer = optim.Adam(generator.parameters(), lr=0.00001)\n",
        "discriminator_optimzer = optim.Adam(discriminator.parameters(), lr=0.00001)\n",
        "for epoch in tqdm(range(EPOCHS), desc='Training'):\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        high_res_real, _ = data\n",
        "        for j in range(BATCHSIZE):\n",
        "            low_res[j] = scale(high_res_real[j])\n",
        "            high_res_real[j] = normalize(high_res_real[j])\n",
        "        high_res_real = high_res_real.to(GeneratorDevice)\n",
        "        high_res_fake = generator(low_res.to(GeneratorDevice))\n",
        "        \n",
        "        target_real = (torch.rand(BATCHSIZE, 1) * 0.5 + 0.7).to(DiscriminatorDevice)\n",
        "        target_fake = (torch.rand(BATCHSIZE, 1) * 0.3).to(GeneratorDevice)\n",
        "        high_res_real = high_res_real.to(DiscriminatorDevice)\n",
        "        high_res_fake = high_res_fake.to(DiscriminatorDevice)\n",
        "\n",
        "        #Train D\n",
        "        discriminator.zero_grad()\n",
        "        discriminator_loss = adversarial_criterion(discriminator(high_res_real), target_real) + adversarial_criterion(discriminator(high_res_fake), target_fake )\n",
        "        discriminator_loss.backward(retain_graph=True)\n",
        "        discriminator_optimzer.step()\n",
        "\n",
        "        #Feature Extractor\n",
        "        real_features = feature_extractor(high_res_real)\n",
        "        fake_features = feature_extractor(high_res_fake)\n",
        "\n",
        "        #Train G\n",
        "        generator.zero_grad()\n",
        "        generator_content_loss = content_criterion(high_res_fake, high_res_real) + 0.006*content_criterion(fake_features, real_features)\n",
        "        generator_adversarial_loss = adversarial_criterion(discriminator(high_res_fake), ones_const)\n",
        "        generator_total_loss = generator_content_loss + 0.001 * generator_adversarial_loss\n",
        "        generator_total_loss.backward()\n",
        "        generator_optimzer.step()\n",
        "\n",
        "\n",
        "    torch.save(generator.state_dict(), './checkpoint/generator_final.pth')\n",
        "    torch.save(discriminator.state_dict(), './checkpoint/discriminator_final.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>Training</span><progress style='margin:2px 4px;description_width:initial;' max='1000' value='0'></progress>  0% 0/1000 [00:00&lt;?, ?it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-24e20eb8e98d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mgenerator_content_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_res_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh_res_real\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.006\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcontent_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mgenerator_adversarial_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_res_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones_const\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mgenerator_total_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_content_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgenerator_adversarial_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mgenerator_total_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2077\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC8CGvUaIMJw"
      },
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        "generator.load_state_dict(torch.load(\"./checkpoint/generator_final.pth\", map_location='cuda:0'))\n",
        "generator.eval()\n",
        "low_res = torch.zeros((1, 3, LOWRES, LOWRES), device=torch.device('cuda:0'))\n",
        "for i, data in enumerate(valid_dataloader):\n",
        "    high_res_real, _ = data\n",
        "    low_res[0] = scale(high_res_real[0])\n",
        "    high_res_fake = generator(low_res)\n",
        "    for j in range(1):\n",
        "        output = unnormalize(high_res_fake[j].cpu()).clamp(min=0, max=1)\n",
        "        save_image(high_res_real, str(i) + '_real.png')\n",
        "        save_image(output, str(i)+'_fake.png')       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84-FjEHGPoZ2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}